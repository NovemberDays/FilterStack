{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d7aee6-fe4b-4ae7-938d-2af3a56195d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set environment variables to limit threading\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"    # Limits OpenMP to one thread\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"  # Limits OpenBLAS to one thread\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"    # Limits MKL (Intel Math Kernel Library) to one thread\n",
    "os.environ[\"VECLIB_MAXIMUM_THREADS\"] = \"1\"  # Limits Apple's Accelerate library to one thread\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"1\"  # Limits NumExpr to one thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd43dda5-3131-4468-80fa-c795b21e6ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_validate\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f8dd47-aeb7-409c-9d02-d46adf6f8a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting number of threads for tensorflow\n",
    "tf.config.threading.set_inter_op_parallelism_threads(1)\n",
    "tf.config.threading.set_intra_op_parallelism_threads(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7110e12a-8cbe-4ffb-b355-6e2d4fca09d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1234\n",
    "np.random.seed(seed)\n",
    "tf.keras.utils.set_random_seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93cadfc-44f2-4787-b945-5b6211db2852",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgb_to_gray(rgb_images):\n",
    "    gray_images = np.dot(rgb_images[...,: 3], [0.299, 0.587, 0.114])\n",
    "    return gray_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7926f1-6577-4079-ae12-68e5302b2b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(directory):\n",
    "    raw_npz = np.load(directory)\n",
    "    X = raw_npz['images']\n",
    "    y = raw_npz['labels']\n",
    "    gray_images = rgb_to_gray(X)\n",
    "    gray_images /= 255.0\n",
    "    return gray_images, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e03e65a-9149-4ecf-bb05-905dbf95b11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_hyp = {\n",
    "    'n_estimators': 10,\n",
    "    'criterion': 'log_loss',\n",
    "    'max_depth': 5,\n",
    "    'random_state': seed,\n",
    "    'n_jobs': 1\n",
    "}\n",
    "\n",
    "svm_hyp = {\n",
    "    'random_state': seed,\n",
    "}\n",
    "\n",
    "dt_hyp = {\n",
    "    'max_depth': 5,\n",
    "    'random_state': seed,\n",
    "    'criterion': 'log_loss'\n",
    "}\n",
    "\n",
    "lr_hyp = {\n",
    "    'random_state': seed,\n",
    "    'n_jobs': 1,\n",
    "}\n",
    "\n",
    "rf_stack_hyp = {\n",
    "    'n_jobs': 1,\n",
    "    'random_state': seed,\n",
    "    'n_estimators': 30,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8a2085-4fde-4bb2-b028-7a1716660e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\n",
    "\t('Dataset/car_bike_raw.npz', 'Car Bike Dataset'),\n",
    "\t('Dataset/cifar10_2_500.npz', 'CIFAR10 Dataset'),\n",
    "\t('Dataset/pizza_raw_32.npz', 'Pizza Dataset'),\n",
    "\t('Dataset/corals.npz', 'Corals Dataset'),\n",
    "\t('Dataset/eggs.npz', 'Eggs Dataset'),\n",
    "\t('Dataset/xray.npz', 'Xray Dataset'),\n",
    "\t('Dataset/covid.npz', 'Covid19 Dataset'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab632a8b-3b5d-4750-9f3b-79147813b3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RK:\n",
    "    def __init__(self, model, weak_hyp, stack_hyp, weak_learner, stack_learner):\n",
    "        self.model = Model(inputs=model.input, outputs=x)\n",
    "        self.weak_hyp = weak_hyp\n",
    "        self.weak_learner = weak_learner\n",
    "        self.stack_learner = stack_learner(**stack_hyp)\n",
    "        print(f'stack learner hyp parameter: {self.stack_learner.get_params()}')\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        num_images = X_train.shape[0]\n",
    "        feature_maps = self.model.predict(X_train, verbose=0)\n",
    "\n",
    "        num_filters = feature_maps.shape[-1]\n",
    "        predictions = []\n",
    "        self.trained_weak_learner = []\n",
    "        for i in range(num_filters):\n",
    "            features = feature_maps[:, :, :, i].reshape(num_images, -1)\n",
    "            weak_learner = self.weak_learner(**self.weak_hyp)\n",
    "            if i == 0:\n",
    "                print(f'weak learner hyp parameter: {weak_learner.get_params()}')\n",
    "            weak_learner.fit(features, y_train)\n",
    "            self.trained_weak_learner.append(weak_learner)\n",
    "            predictions.append(weak_learner.predict(features))\n",
    "        stacked_predictions = np.stack(predictions, axis=1)\n",
    "        self.stack_learner.fit(stacked_predictions, y_train)\n",
    "        return self.stack_learner.predict(stacked_predictions)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        num_images = X.shape[0]\n",
    "        feature_maps = self.model.predict(X, verbose=0)\n",
    "\n",
    "        num_filters = feature_maps.shape[-1]\n",
    "        predictions = []\n",
    "        for i in range(num_filters):\n",
    "            features = feature_maps[:, :, :, i].reshape(num_images, -1)\n",
    "            predictions.append(self.trained_weak_learner[i].predict(features))\n",
    "        stacked_predictions = np.stack(predictions, axis=1)\n",
    "        return self.stack_learner.predict(stacked_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af57feb-d8f3-4bbe-bdb7-7d921c148b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = tf.keras.Input(shape=(32, 32, 1))\n",
    "conv_layer = layers.Conv2D(100, (3, 3), activation='relu')(input_layer)\n",
    "x = layers.MaxPooling2D((2, 2))(conv_layer)\n",
    "model = Model(inputs=input_layer, outputs=x)\n",
    "model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d18082-2bd6-4db8-9704-517acaa8821e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv(directory, weak_hyp, stack_hyp, weak_learner, stack_learner):\n",
    "    X, y = get_dataset(directory)\n",
    "    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "    \n",
    "    results = {\n",
    "        'model_name': [],\n",
    "        'directory': [],\n",
    "        'train_acc': [],\n",
    "        'test_acc': [],\n",
    "        'train_prec': [],\n",
    "        'test_prec': [],\n",
    "        'train_recall': [],\n",
    "        'test_recall': [],\n",
    "        'train_time': [],\n",
    "        'test_time': []\n",
    "    }\n",
    "\n",
    "    for i, (train_indices, test_indices) in enumerate(kf.split(X, y)):\n",
    "        X_train, X_test, y_train, y_test = X[train_indices], X[test_indices], y[train_indices], y[test_indices]\n",
    "        \n",
    "        # Start of training time\n",
    "        start_train_time = time.time()\n",
    "        clf = RK(model, weak_hyp, stack_hyp, weak_learner, stack_learner)\n",
    "        pred_train = clf.fit(X_train, y_train)\n",
    "        end_train_time = time.time()\n",
    "        train_time = end_train_time - start_train_time\n",
    "        results['train_time'].append(train_time)\n",
    "        # End of training, beginning of evaluation/test time\n",
    "        \n",
    "        start_test_time = time.time()\n",
    "        pred_test = clf.predict(X_test)\n",
    "        end_test_time = time.time()\n",
    "        test_time = end_test_time - start_test_time\n",
    "        # End of test time\n",
    "        results['test_time'].append(test_time)\n",
    "\n",
    "        results['train_acc'].append(accuracy_score(y_train, pred_train))\n",
    "        results['test_acc'].append(accuracy_score(y_test, pred_test))\n",
    "\n",
    "        results['train_prec'].append(precision_score(y_train, pred_train))\n",
    "        results['test_prec'].append(precision_score(y_test, pred_test))\n",
    "\n",
    "        results['train_recall'].append(recall_score(y_train, pred_train))\n",
    "        results['test_recall'].append(recall_score(y_test, pred_test))\n",
    "        \n",
    "        # Add directory for each row\n",
    "        results['directory'].append(directory)\n",
    "        \n",
    "        results['model_name'].append(f'{weak_learner.__name__}_{stack_learner.__name__}')\n",
    "\n",
    "    df_results = pd.DataFrame(results)\n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22175d92-679b-4922-96d4-36610dcccac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_save(weak_hyp, stack_hyp, weak_learner, stack_learner):\n",
    "    X1, y1 = get_dataset('Dataset/covid.npz')\n",
    "    X2, y2 = get_dataset('Dataset/car_bike_raw.npz')\n",
    "    fs1 = RK(model, weak_hyp, stack_hyp, weak_learner, stack_learner)\n",
    "    fs1.fit(X1, y1)\n",
    "    \n",
    "    fs2 = RK(model, weak_hyp, stack_hyp, weak_learner, stack_learner)\n",
    "    fs2.fit(X2, y2)\n",
    "    \n",
    "    pickle.dump(fs1, open(f'VariantsSingleThread/filter_stack_{str(weak_learner)}_{str(stack_learner)}_S', 'wb'))\n",
    "    pickle.dump(fs2, open(f'VariantsSingleThread/filter_stack_{str(weak_learner)}_{str(stack_learner)}_L', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c79990-b6bc-40ce-a899-3a4d2ce043f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dfs = []\n",
    "for dataset, _ in datasets:\n",
    "    print(dataset)\n",
    "    df = cv(dataset, rf_hyp, svm_hyp, RandomForestClassifier, SVC)\n",
    "    all_dfs.append(df)\n",
    "\n",
    "merged_df = pd.concat(all_dfs, ignore_index=True)\n",
    "merged_df.to_csv('VariantsCVSingleThread/merged_results_rf_svm.csv', index=False)\n",
    "print(\"All results have been merged and saved to merged_results.csv\")\n",
    "model_save(rf_hyp, svm_hyp, RandomForestClassifier, SVC)\n",
    "print('Models pickled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c456b01d-91ed-4040-8ccc-efff72e6f049",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dfs = []\n",
    "for dataset, _ in datasets:\n",
    "    print(dataset)\n",
    "    df = cv(dataset, dt_hyp, rf_stack_hyp, DecisionTreeClassifier, RandomForestClassifier)\n",
    "    all_dfs.append(df)\n",
    "\n",
    "merged_df = pd.concat(all_dfs, ignore_index=True)\n",
    "merged_df.to_csv('VariantsCVSingleThread/merged_results_dt_rf.csv', index=False)\n",
    "print(\"All results have been merged and saved to merged_results.csv\")\n",
    "model_save(dt_hyp, rf_stack_hyp, DecisionTreeClassifier, RandomForestClassifier)\n",
    "print('Models pickled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7c077e-2498-4a49-a90d-69c48d89e1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dfs = []\n",
    "for dataset, _ in datasets:\n",
    "    print(dataset)\n",
    "    df = cv(dataset, dt_hyp, svm_hyp, DecisionTreeClassifier, SVC)\n",
    "    all_dfs.append(df)\n",
    "\n",
    "merged_df = pd.concat(all_dfs, ignore_index=True)\n",
    "merged_df.to_csv('VariantsCVSingleThread/merged_results_dt_svm.csv', index=False)\n",
    "print(\"All results have been merged and saved to merged_results.csv\")\n",
    "model_save(dt_hyp, svm_hyp, DecisionTreeClassifier, SVC)\n",
    "print('Models pickled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90399718-21db-4a48-84c9-3bfc0665f7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dfs = []\n",
    "for dataset, _ in datasets:\n",
    "    print(dataset)\n",
    "    df = cv(dataset, rf_hyp, rf_stack_hyp, RandomForestClassifier, RandomForestClassifier)\n",
    "    all_dfs.append(df)\n",
    "\n",
    "merged_df = pd.concat(all_dfs, ignore_index=True)\n",
    "merged_df.to_csv('VariantsCVSingleThread/merged_results_rf_rf.csv', index=False)\n",
    "print(\"All results have been merged and saved to merged_results.csv\")\n",
    "model_save(rf_hyp, rf_stack_hyp, RandomForestClassifier, RandomForestClassifier)\n",
    "print('Models pickled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f616e637-f458-4746-9a25-1e76ec6c78bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dfs = []\n",
    "for dataset, _ in datasets:\n",
    "    print(dataset)\n",
    "    df = cv(dataset, rf_hyp, lr_hyp, RandomForestClassifier, LogisticRegression)\n",
    "    all_dfs.append(df)\n",
    "\n",
    "merged_df = pd.concat(all_dfs, ignore_index=True)\n",
    "merged_df.to_csv('VariantsCVSingleThread/merged_results_rf_lr.csv', index=False)\n",
    "print(\"All results have been merged and saved to merged_results.csv\")\n",
    "model_save(rf_hyp, lr_hyp, RandomForestClassifier, LogisticRegression)\n",
    "print('Models pickled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eab149d-1463-44d8-9a3f-d5040f0b560c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dfs = []\n",
    "for dataset, _ in datasets:\n",
    "    print(dataset)\n",
    "    df = cv(dataset, dt_hyp, lr_hyp, DecisionTreeClassifier, LogisticRegression)\n",
    "    all_dfs.append(df)\n",
    "\n",
    "merged_df = pd.concat(all_dfs, ignore_index=True)\n",
    "merged_df.to_csv('VariantsCVSingleThread/merged_results_dt_lr.csv', index=False)\n",
    "print(\"All results have been merged and saved to merged_results.csv\")\n",
    "model_save(dt_hyp, lr_hyp, DecisionTreeClassifier, LogisticRegression)\n",
    "print('Models pickled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91ef66a-260d-4dd7-8c30-27e8963afdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702551ff-b1b2-43b5-b766-1bf2ca374685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path for all CSV files\n",
    "path = \"VariantsCVSingleThread/*.csv\"\n",
    "all_files = glob.glob(path)\n",
    "\n",
    "df_list = []\n",
    "\n",
    "# Loop through all files and read them into a list of DataFrames\n",
    "for file in all_files:\n",
    "    df = pd.read_csv(file)\n",
    "    df_list.append(df)\n",
    "\n",
    "# Concatenate all DataFrames into a single DataFrame\n",
    "merged_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# Save the merged DataFrame to a new CSV file\n",
    "merged_df.to_csv(\"VariantsCVSingleThread/merged_df.csv\", index=False)\n",
    "\n",
    "print(\"Files merged successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
